{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Analysis: Computational Methods - Summer Term 2025\n",
    "### Lectures: Jun.-Prof. Dr. Andreas Spitz\n",
    "### Tutorials: Julian Schelb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will learn about:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenization\n",
    "- data cleaning and stop word removal\n",
    "- stemming\n",
    "- zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "Describe what tokenization is, how it is performed, and what problems it solves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 100 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "The texts.json file contains three datasets called \"debates\", \"reddit\", and \"tv\".\n",
    "\n",
    "For the later analysis of each dataset, we need to identify single tokens. Therefore, you have to use a library to separate single tokens from the text. We will use the methods offered by `nltk` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/texts.json', 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "content_debates = data['debates']\n",
    "content_reddit = data['reddit']\n",
    "content_tv = data['tv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenize the text content for the three datasets above\n",
    "# 2. Print the first 20 tokens for each dataset\n",
    "# 3. Now display the first paragraphs of the corresponding original text and study them.\n",
    "#    (do _not_ use print(), but just the debug view of jupyter notebook which retains control characters) \n",
    "\n",
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Does this what you expected it to do? How well does the tokenization work? What happens to special characters? Can you think of any problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 100-200 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Data Cleaning and Stop Word Removal:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "In two to three sentences, describe what *data cleaning* in the context of text data refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected 2-3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "To have more accurate word counts and visualizations, it is often helpful to remove the capitalization of words. This is especially true for languages like German. In the following, for the three texts from above, remove any capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Apply tokenization to the lowercase version of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "In two to three sentences, describe what *stop word removal* in the context of text data refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected 2-3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "Now apply stop word removal to the three datasets.\n",
    "\n",
    "Hint: Assume the texts are all written in _English_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "\n",
    "Now compare the first original sentence for each dataset with the parts remaining after performing the above steps. Write them down and explain what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 50-100 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Stemming:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "In two to three sentences, describe what *stemming* in the context of text data refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected 2-3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Think about how you would go about implementing your own stemmer?\n",
    "Come up with at least ten rules and write them down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: For example:\n",
    "\n",
    "```*s -> *   # remove trailing s```\n",
    "\n",
    "```*s -> *   # remove trailing s```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 50-100 words including the rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Use the cleaned word tokens (Step 5 above) and apply stemming. Use the Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "Compare the results of the Snowball Stemmer with your stemming rules. How do they differ, how could you improve your stemmer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 100-200 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "Create the word clouds from ex00 again, but now with the preprocessed text.\n",
    "(Remark: You can use the *create_word_cloud* method provided in the *utils.py*)\n",
    "\n",
    "What changes do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_word_cloud\n",
    "\n",
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 100 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Zipf's Law:\n",
    "\n",
    "In the lecture, you have heard about Zipfâ€™s law. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "State Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 1-2 sentences and the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Check if Zipf's law (approximately) holds for our three datasets after all preprocessing steps.\n",
    "\n",
    "For this, plot Zipf's law and the word distribution for each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)\n",
    "plt.plot(np.linspace(0, 5000, 5000), [np.log(x) for x in range(5000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Describe your plots and discuss your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 50-100 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting your results:\n",
    "\n",
    "To submit your results, please:\n",
    "\n",
    "- save this file, i.e., `ex??_assignment.ipynb`.\n",
    "- if you reference any external files (e.g., images), please create a zip or rar archieve and put the notebook files and all referenced files in there.\n",
    "- login to ILIAS and submit the `*.ipynb` or archive for the corresponding assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "    \n",
    "- Do not copy any code from the Internet. In case you want to use publicly available code, please, add the reference to the respective code snippet.\n",
    "- Check your code compiles and executes, even after you have restarted the Kernel.\n",
    "- Submit your written solutions and the coding exercises within the provided spaces and not otherwise.\n",
    "- Write your name in the top section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
